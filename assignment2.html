<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Ashley Lee</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800'
        rel='stylesheet' type='text/css'>

    <!-- Custom styles for this template -->
    <link href="css/clean-blog.min.css" rel="stylesheet">

</head>

<body>

    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
        <div class="container">
            <a class="navbar-brand" href="about.html">Ashley Lee</a>
            <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive"
                aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                Menu
                <i class="fa fa-bars"></i>
            </button>
            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="about.html">About Me</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="s198.html">6.S198</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Page Header -->
    <header class="masthead" style="background-image: url('img/post-bg.jpg')">
        <div class="overlay"></div>
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-md-10 mx-auto">
                    <div class="post-heading">
                        <h1>Assignment 2</h1>
                        <h2 class="subheading">Teachable Machines</h2>
                        <span class="meta">Posted on September 17, 2018</span>
                    </div>
                </div>
            </div>
        </div>
    </header>

    <!-- Post Content -->
    <article>
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-md-10 mx-auto">

                    <h2 class="post-subtitle"> Section 1.1 Building Models with Model Builder </h2>
                    <img class="img" src="pset2/1_1.png" width="100%">
                    <p>The initial model shown above is marked "invalid" because while the shape of input layer is [28, 28,
                        1], the Softmax Cross Entropy layer in the middle expects shape of [10]. Because the size of the
                        data doesn't match in the two consecutive layers, the model is invalid.</p>

                    <h2 class="post-subtitle"> Problem 1 </h2>
                    <p> Initially, the classification shown in Inference section is almost always wrong. This happens mostly
                        because the parameters of the layers (weight and bias) are assigned randomply upon initialization.
                        While the weights and biases are initially set to random values and gets updated through training,
                        since the current model lacks training, the weights' initial random values can't get updated. Moreover,
                        the 10 classification from 0 to 9, are also assigned randomly, which implies that probability of
                        labeling the image correcly is 1/10.
                    </p>

                    <h2 class="post-subtitle"> Problem 2 </h2>
                    <img class="img" src="pset2/2_1.png" width="100%">
                    <p>1. Training MNIST and Fashion MNIST</p>
                    <p>Image above is a snapshot after training MNIST for 5148 images, which took around 6.9 seconds. As
                        demonstrated in the image, the accuracy went dramatically higher compare to the previous model as
                        a result of training. While the observed accuracy ranges from 6.4% to 100%, in average, the accuracy
                        is arround 86.7%. The model trained around 1210 examples per second and performed 1330 inferences
                        per second.
                    </p>
                    <p>The two images below show the result of traning Fashion MNIST.
                    </p>
                    <img class="img" src="pset2/2_1_f1.png" width="80%">
                    <img class="img" src="pset2/2_1_f2.png" width="80%">

                    <p>The training stats of training Fashion MNIST and MNIST were largely similar. For both data sets, the
                        model trained around 5000 examples -- 5148 examples for MNIST and 5440 examples for Fashion MNIST.
                        The speed and duration of the trainings were also similar, since training MNIST took 6.9 seconds
                        with 1210 examples per second and training Fashion MNIST took 6.7 seconds in total with 1250 examples
                        per second.
                    </p>
                    <p>
                        On the other hand, the result of inference was noticeably different. First, while the demo performed 1330 inferences per
                        second after training MNIST, it was only able to compute around 1044 inferences per second after
                        training Fashion MNIST. Moreover, the estimated accuracy was significantly different as well. After
                        training MNIST for around 5148 examples, the model had observed accuracy of 86.7% in average, which
                        is fairly high. Yet, despite the fact that the demo trained Fashion MNIST for 5440 examples, around
                        300 more examples compared to the previous training, the observed accuracy was lower. There were
                        few items that had high accuracy, but in average, the observed accuracy was around 50~60%. This difference
                        can be easily observed by comparing the figures above as well.
                    </p>

                    <p>2. CIFAR-10 Dataset</p>
                    <img class="img" src="pset2/2_2.png" width="100%">
                    <p>Image above shows the result of training CIFAR-10 Dataset.</p>
                    <p>As suggested by the problem, CIFAR was trained for 90 seconds, during which 56960 examples were trained, with average speed of 1330 examples per second.
                        However, despite the longer training, the observed accuracy was very low. Majority of items were guessed incorreclty, and the observed accuracy was around 45%. 
                        Furthermore, the model could only compute 801 inferences per second, which is even slower than the result of training Fashion MNIST.
                    </p>

                    <p>3. Adding more fully connected layers</p>
                    <p>Switching back to the MNIST dataset, 3 more fully connected layers were added to the model and 5312 examples were trained.</p>
                    <img class="img" src="pset2/2_3.png" width="100%">
                    <p> This model didn't work since the accuracy was NaN (not a number). 
                        This can be attributed to the fact that cascading linear layers will eventually explode, since adding a linear layer only adds another linear operation.
                    </p>

                    <p>4. Input → Flatten → FC(10) → FC(10) → Softmax → Label</p>
                    <img class="img" src="pset2/2_4.png" width="100%">
                    <p> As suggested, the accuracy plummetted to 0%. As explained in the previous question, placing fully connected layers consecutively only causes the model to diverge. 
                        In order to fix the issue, a nonlinear activation function should be added. 
                    </p>

                    <h2 class="post-subtitle"> Problem 3 Adding Activation Layer </h2>
                    <img class="img" src="pset2/3_11.png" width="100%">
                    <img class="img" src="pset2/3_12.png" width="50%">
                    <p>While adding an activation function (ReLU) solved the problem of adding linear layers consecutively, the observed accuracy was low with average 14%.</p>
                    <img class="img" src="pset2/3_21.png" width="100%">
                    <img class="img" src="pset2/3_22.png" width="50%">
                    <p>As shown in the two images above, making hte first fully connected model wider definitely improved the accuracy. 
                        After increasing the number of units to 100, the accuracy increased up to around 85%, when similar number of examples were trained. 
                        Also, we can notice that adding an activation layer decreased the speed of training to around 830 examples per second.
                    </p>

                    <h2 class="post-subtitle"> Problem 4 </h2>
                    <img class="img" src="pset2/3_11.png" width="100%">
                    <img class="img" src="pset2/3_12.png" width="50%">
                    <p>While adding an activation function (ReLU) solved the problem of adding linear layers consecutively, the observed accuracy was low with average 14%.</p>
                    <img class="img" src="pset2/3_21.png" width="100%">
                    <img class="img" src="pset2/3_22.png" width="50%">
                    <p>As shown in the two images above, making hte first fully connected model wider definitely improved the accuracy. 
                        After increasing the number of units to 100, the accuracy increased up to around 85%, when similar number of examples were trained. 
                        Also, we can notice that adding an activation layer decreased the speed of training to around 830 examples per second.
                    </p>

                </div>
            </div>
        </div>
    </article>

    <hr>

    <!-- Footer -->
    <footer>
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-md-10 mx-auto">
                    <ul class="list-inline text-center">
                        <li class="list-inline-item">
                            <a href="https://www.linkedin.com/in/ashley-jieun-lee/">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                        <li class="list-inline-item">
                            <a href="https://github.com/ajlee19">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                    </ul>
                    <p class="copyright text-muted">Copyright &copy; Ashley JiEun Lee</p>
                </div>
            </div>
        </div>
    </footer>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/clean-blog.min.js"></script>

</body>

</html>